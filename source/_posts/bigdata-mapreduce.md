---
title: 大数据系列——MapReduce分布式并行计算框架
date: 2020-11-01 13:08:46
tags: 大数据
---

讨论MapReduce之前，我们先从一个简单问题入手，一步步叙述MapReduce原理。

# 问题

我们有2000个文件共50TB，每一行内容都是数字，现在需要对这些数字做排序，使用什么方法可以简单高效的完成任务呢？由于文件太大，不能全部载入内存来做排序，而且一台机器的磁盘也很难容纳这么大的文件，可以使用多台机器并行执行排序任务，可快速的完成任务。使用多台机器并行计算时，需要控制哪些机器处理哪些数据。50TB共2000个文件，平均每个文件也有25.6GB，因此也不能把一个文件全部载入内存做排序。可以先将一个文件按照64MB对其切割，每次读入一个64MB的切片在内存里做排序，然后将排序结果写到一个中间文件，等到一个文件的全部切片都排序完成，此时磁盘已经有很多个已排序好的中间文件。我们可以再使用归并排序将这些有序切片文件合并生成一个大的有序文件，这样一来总共就有2000个有序的中间文件。由于这2000个有序中间文件只是局部有序，还需要对这2000个有序中间文件再次使用归并排序，输出多个有序的最终文件。

针对这个排序的问题，我们先去掉业务相关的排序功能，抽象通用功能，发现：

* 文件切割；
* 将数据送个业务去处理生成中间文件；
* 合并中间文件再次送给业务去处理；
* 控制任务分配，哪些机器跑哪些任务；
* 分布式文件系统；

这些都属于通用功能，具体的业务处理代码就是分布式并计算中留给用户实现的代码。这种模式由Google于2003年开发出来并公布论文[MapReduce : Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)，Hadoop也是基于这篇论文实现的MapReduce。

# 原理

受到函数式编程语言Lisp的启发，将复杂业务场景使用简单的map和reduce两个函数来实现，map计算生成中间结果，合并相同key的数据，reduce对map结果再次处理，得到最终结果，可表示为图-1。

![MapReduce类型](/images/mapreduce_types.png "MapReduce类型")

*图-1 MapReduce类型*

先来看看MapReduce的执行流程，如图-2所示。

![MapReduce执行流程](/images/mapreduce_execution_diagram.png "MapReduce执行流程")

*MapReduce执行流程*

每一步执行的操作是：

(0) MapReduce框架首先将输入文件切分为**逻辑上**的多个分片文件，分片文件是MapReduce对文件进行处理和运算的输入单位。因为是逻辑分片，物理上并不会切分，只是记录了要处理数据的位置和长度。

(1) Master将空闲的map任务分配在一台机器上执行。Master负责管理map和reduce任务，通过ping的方式来检查任务健康行为。如果在规定时间内任务没有返回检查状态，Master认为该任务已经死了，需要将该任务重置为空闲状态，然后重新分配执行。分片文件一般与Map任务同在一台机器或者在一个机房，叫做“计算向数据考虑”，可以节省数据网络传输的时间。

(2) Map任务读取分片文件的数据，将输入的(k1,v1)送给用户实现的代码，用户代码输出(k2,v2)，输出的数据会直接写入内存缓冲区。

(3) 内存缓冲区存放map输出数据(k2,v2)。

(4) 当内存缓冲区写到一定程度时，后端线程会将内存数据按照`hash(k2) mod R`来分区并在内存中排序然后写入到临时的分区文件。随着内存数据不断的写入，磁盘会存在多个分区文件，比如分区编号为0的分区文件可能存在几十个，这时还需要使用归并排序，合并成一个文件。这种行为称为溢写、分区、排序、合并。当Map任务完成时，Map需要将这些文件的位置信息上报到Master节点，Master根据这些信息安排Reduce任务从哪里取数据。

(5) 当所有Map任务执行完毕时，Master分配Reduce任务，准备从远端读取Map中间有序分区文件。

(6) Reduce任务通过RPC从多个Map获取多个分区文件。

(7) 由于分区文件来自不同的Map，多个文件只是局部有序，需要使用类似第4步的溢写、排序、归并排序，合并成一个大的有序文件。

(8) Reduce任务从有序文件读取(k2,list(v2))数据集，交给用户实现的reduce逻辑。

(9) 用户实现的reduce逻辑会输出list(v2)到最终文件。在输出到最终文件之前，同样会使用类似第4步溢写、归并排序生成一个大文件。当reduce任务结束时，向Master上报任务执行状态。

# 容错性

(1) map任务的容错性：如果master通过ping检查机制发现map任务挂掉了，会将该任务重置为空闲状态，空闲状态的map任务会被master重新分配来执行新的计算。如果一台机器挂掉了，master发现该机器存储了map已完成的中间有序文件，而且还没有被reduce任务领取，master会重新分配map任务重跑这些数据。因为map的中间文件都是本地存储，只有一个副本，机器挂掉了不能恢复时，文件内容也不能恢复。如果master受到了重复上报的已完成的中间有序文件，master会忽略这个信息，只保留一份。

(2) reduce容错性：如果master通过ping检查机制发现reduce任务挂掉了，会将该任务重置为空闲状态，空闲状态的reduce任务会被master重新分配来执行新的计算。由于reduce输出的文件存放在分布式文件系统，比如GFS或者HDFS，有多个副本，就算已完成的reduce任务所在的机器挂掉了，也不需要重新跑。

(3) Master的容错性：Master内部有检查点机制，检查点周期性的保存到磁盘，如果master挂掉，恢复时会从上一份检查点开始再次执行任务，这样会有部分数据的丢失，丢失的信息需要重跑补回来。一开始Master节点是单节点，无法做到高可用。后来在Hadoop中引入了备用的master节点，实时从活动的Master节点同步数据，避免单点故障。

(4) Semantics in the Presence of Failures：MapReduce使用map和reduce的原子提交来达到这种特征。每一个任务输出的文件首先是临时文件，比如reduce任务是一个输出文件，map任务是R个输出文件。如果任务完成，使用rename的原子操作将文件重命名并提交到master节点，master节点会忽略重复的信息。如果多个任务执行同样的rename操作，只有一个rename能成功，因为文件只能有一个。

# 应用

除了文中开头说的排序可以使用MapReduce来完成外，MapReduce还可以使用在关系代数运算、分组与聚合运算等。

关系代数运算包括

(1) 关系的选择运算：对于关系的选择运算，只需要Map过程就能实现，对于关系R中的每个元组t，检测是否是满足条件的所需元组，如果满足条件，则输出键值对(t,t)，也就是说，键和值都是t。这时的Reduce函数就是一个恒等式，对输入不作任何变换就直接输出。

(2) 关系的投影运算：假设对关系R投影后的属性集为S，在Map函数中，对于R中的每个元组t，提出t中不属于S的字段，得到元组t<sup>'</sup>，输出键值对(t<sup>'</sup>,t<sup>'</sup>)。对于Map任务产生的每个键t<sup>'</sup>，都可能存在一个或者多个键值对(t<sup>'</sup>,t<sup>'</sup>)，因此需要通过Reduce函数来剔除冗余，把属性完全相同的元组合并起来的到(t<sup>'</sup>,list(t<sup>'</sup>))，剔除冗余后只输出一个(t<sup>'</sup>,t<sup>'</sup>)。

(3) 关系的并、交、差运算。

(4) 关系的自然连接运算：将需要连接的公共字段作为键。



